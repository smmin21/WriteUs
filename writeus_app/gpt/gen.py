# -*- coding: utf-8 -*-
"""gen.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FZZgGOOe07VfdMB294z5HOCzIIHjLgsp
"""

class Generation:
    def __init__(self):

        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM 

        self.tokenizer = AutoTokenizer.from_pretrained(
          'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b',  # or float32 version: revision=KoGPT6B-ryan1.5b
          bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]', mask_token='[MASK]'
        )

        self.model = AutoModelForCausalLM.from_pretrained(
          'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b',  # or float32 version: revision=KoGPT6B-ryan1.5b
          pad_token_id=self.tokenizer.eos_token_id,
          torch_dtype='auto', low_cpu_mem_usage=True
        ).to(device='cuda', non_blocking=True)

        _ = self.model.eval()
    
    def text_split(self, generated):
        line = generated.split()
        parse = []
        pre = 0
        deli = ['.', '?', '!']

        for i in range(len(line)):
            if line[i][-1] in deli:
                parse.append(" ".join(line[pre:i+1]))
                pre = i+1

        generated_list = parse
        return generated_list

    def text_generator(self, seed="", DB=""):
        import torch
        history = ""

        data = self.text_split(DB)

        if len(data) < 4:
            history = DB
        else:
            history = " ".join(data[-3:])
        print("history: ", history)
        prompt = "소설 작성: " + history + " " + seed
        prompt = prompt.strip()

        num = len(self.text_split(history))+1
        with torch.no_grad():
            tokens = self.tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)
            gen_tokens = self.model.generate(tokens, do_sample=True, temperature=0.9, max_length=128)
            generated = self.tokenizer.batch_decode(gen_tokens)[0]

            generated_list = self.text_split(generated)

            try: 
                generated = generated_list[num]
            except:
                while len(generated_list) <= num:
                    tokens = self.tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)
                    gen_tokens = self.model.generate(tokens, do_sample=True, temperature=0.9, max_length=128)
                    generated = self.tokenizer.batch_decode(gen_tokens)[0]

                    generated_list = self.text_split(generated)
                generated = generated_list[num]
        return generated